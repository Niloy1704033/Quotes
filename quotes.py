# -*- coding: utf-8 -*-
"""Niloy_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11v7SSKekIhTknDRtcR5M_W80hKgG1Pg0
"""

# Install required libraries
!pip install -q transformers datasets scikit-learn joblib

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
import joblib

path="/content/drive/MyDrive/Dataset/quotes.csv"
df=pd.read_csv(path)
df.head(5)

# Step 1: Load and preprocess the dataset
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
import joblib

# Replace 'path' with your actual file name
data = pd.read_csv(path)  # Make sure 'quotes.csv' has 'quote' and 'tags' columns

# Handle missing or non-string values safely
data['tags'] = data['tags'].fillna('').apply(lambda x: x.split(',') if isinstance(x, str) else [])

# Optionally filter out rows with no tags
data = data[data['tags'].map(len) > 0]

# Fit and transform using MultiLabelBinarizer
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(data['tags'])

# Save the label encoder
joblib.dump(mlb, 'bert_label_binarizer.pkl')

from transformers import BertTokenizer

from transformers import BertTokenizer  # ✅ Add this import
from torch.utils.data import Dataset, DataLoader
import torch

# === Step 2: Tokenizer and Dataset ===
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class QuoteDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = tokenizer(
            self.texts[idx],
            padding='max_length',
            truncation=True,
            max_length=128,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.FloatTensor(self.labels[idx])
        }

from transformers import BertForSequenceClassification

from transformers import BertForSequenceClassification  # ✅ Import this
from torch.optim import AdamW
import torch

# Define the model
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=y.shape[1],
    problem_type='multi_label_classification'
)

model.train()

# Optimizer setup
optimizer = AdamW(model.parameters(), lr=5e-5)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

# Assume 'data' and 'y' are already prepared from preprocessing step
X_train, X_test, y_train, y_test = train_test_split(data['quote'], y, test_size=0.2, random_state=42)

# Dataset class (redefine if not already present)
class QuoteDataset(Dataset):
    def __init__(self, quotes, labels):
        self.quotes = quotes
        self.labels = labels

    def __len__(self):
        return len(self.quotes)

    def __getitem__(self, idx):
        encoding = tokenizer(
            self.quotes[idx],
            padding='max_length',
            truncation=True,
            max_length=128,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.FloatTensor(self.labels[idx])
        }

# Create dataset and loader
train_dataset = QuoteDataset(X_train.tolist(), y_train)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

from tqdm import tqdm  # progress bar

epochs = 3
model.train()

for epoch in range(epochs):
    total_loss = 0
    loop = tqdm(train_loader, leave=True, desc=f"Epoch {epoch+1}")

    for batch in loop:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())

    print(f"Epoch {epoch+1} average loss: {total_loss / len(train_loader):.4f}")

# Save the model
model.save_pretrained("bert_quote_tag_model")
tokenizer.save_pretrained("bert_quote_tag_model")

# Step 5: Prediction Function
def predict_tags_bert(quote):
    model = BertForSequenceClassification.from_pretrained("bert_quote_tag_model")
    tokenizer = BertTokenizer.from_pretrained("bert_quote_tag_model")
    mlb = joblib.load('bert_label_binarizer.pkl')
    model.to(device)
    model.eval()

    inputs = tokenizer(quote, return_tensors='pt', padding='max_length', truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]

    threshold = 0.5
    predicted = (probs >= threshold).astype(int)
    return mlb.inverse_transform(np.array([predicted]))[0]

# === Example usage ===
if __name__ == '__main__':
    test_quote = "Don't cry because it's over, smile because it happened."
    tags = predict_tags_bert(test_quote)
    print("Predicted tags:", tags)